# ğŸ“– Theory Notes: Gradient Descent & Optimization

> **CS231n Lecture 3:** Loss Functions & Optimization  
> **Author:** Hamza  
> **Last Updated:** November 2024

---

## ğŸ¯ Table of Contents

1. [The Optimization Problem](#1-the-optimization-problem)
2. [Gradient Descent: Mathematical Foundation](#2-gradient-descent-mathematical-foundation)
3. [The Three Variants: BGD, SGD, MBGD](#3-the-three-variants)
4. [Learning Rate Dynamics](#4-learning-rate-dynamics)
5. [Convergence Analysis](#5-convergence-analysis)
6. [Common Pitfalls & Solutions](#6-common-pitfalls--solutions)
7. [Connection to Modern Optimizers](#7-connection-to-modern-optimizers)

---

## 1. The Optimization Problem

### **Problem Formulation**

Given:
- Training dataset: `D = {(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (xâ‚™, yâ‚™)}`
- Model: `f(x; W)` parameterized by weights `W`
- Loss function: `L(W)` that measures prediction error

**Goal:** Find weights `W*` that minimize average loss

```
W* = argmin_W L(W)

Where: L(W) = (1/N) Î£áµ¢ L(f(xáµ¢; W), yáµ¢)
```

### **Why is This Hard?**

1. **Non-convex loss surface:** Neural networks have many local minima
2. **High dimensionality:** Modern networks have millions of parameters
3. **Computational cost:** Computing exact gradient uses entire dataset

### **The Solution: Gradient Descent**

Instead of finding the global minimum directly, **iteratively move downhill**.

---

## 2. Gradient Descent: Mathematical Foundation

### **The Core Idea**

**Intuition:** If you're blindfolded on a mountain, walk downhill to reach the valley.

**Mathematical formulation:**

The gradient `âˆ‡L(W)` points in the direction of **steepest increase**. To **decrease** loss, move in the **opposite direction**.

### **Update Rule**

```
W_{t+1} = W_t - Î± Â· âˆ‡L(W_t)

Where:
  W_t     = weights at step t
  Î±       = learning rate (step size)
  âˆ‡L(W_t) = gradient of loss at current weights
```

### **Why Does This Work?**

**First-order Taylor approximation:**

```
L(W + Î”) â‰ˆ L(W) + âˆ‡L(W)áµ€ Â· Î”
```

If we choose `Î” = -Î± Â· âˆ‡L(W)`:

```
L(W - Î±âˆ‡L) â‰ˆ L(W) - Î± Â· ||âˆ‡L(W)||Â²
                        â†‘
                   Always positive!
```

Therefore: `L(W - Î±âˆ‡L) < L(W)` for small enough `Î±`

**Conclusion:** Moving in the negative gradient direction **guarantees** loss decrease (locally).

---

### **Gradient Computation**

For a loss function `L(W)`, the gradient is a vector of partial derivatives:

```
âˆ‡L(W) = [âˆ‚L/âˆ‚wâ‚, âˆ‚L/âˆ‚wâ‚‚, ..., âˆ‚L/âˆ‚wâ‚™]
```

**Example: MSE Loss**

```
L(W) = (1/N) Î£áµ¢ (xáµ¢áµ€W - yáµ¢)Â²

âˆ‡L(W) = (2/N) Î£áµ¢ (xáµ¢áµ€W - yáµ¢) Â· xáµ¢
      = (2/N) Xáµ€(XW - y)
```

**Chain rule in action:**
```
dL/dW = dL/dÅ· Â· dÅ·/dW
      = 2(Å· - y) Â· x
```

---

## 3. The Three Variants

The key difference: **how many examples to use when computing gradient?**

### **A. Batch Gradient Descent (BGD)**

**Definition:** Use **all N training examples** to compute gradient

```python
for epoch in range(num_epochs):
    gradient = 0
    for i in range(N):  # ALL examples
        gradient += compute_gradient(x[i], y[i], W)
    gradient /= N
    W = W - learning_rate * gradient
```

**Mathematical form:**
```
âˆ‡L(W) = (1/N) Î£áµ¢â‚Œâ‚á´º âˆ‡Láµ¢(W)
```

**Pros:**
- âœ… Exact gradient (no estimation error)
- âœ… Smooth, deterministic convergence
- âœ… Guaranteed convergence for convex functions

**Cons:**
- âŒ Slow: Must process all N examples before one update
- âŒ Memory intensive: Need to load entire dataset
- âŒ Stuck in local minima (no exploration)

**When to use:** Small datasets (N < 10,000), convex problems

---

### **B. Stochastic Gradient Descent (SGD)**

**Definition:** Use **one random example** to compute gradient

```python
for epoch in range(num_epochs):
    shuffle(data)
    for i in range(N):  # ONE example at a time
        gradient = compute_gradient(x[i], y[i], W)
        W = W - learning_rate * gradient
```

**Mathematical form:**
```
âˆ‡L(W) â‰ˆ âˆ‡Láµ¢(W)  (gradient from random example i)
```

**Key property: Unbiased estimator**
```
E[âˆ‡Láµ¢] = (1/N) Î£áµ¢ âˆ‡Láµ¢ = âˆ‡L  (true gradient)
```

**Pros:**
- âœ… Fast updates: One example â†’ one update
- âœ… Memory efficient
- âœ… Can escape local minima (noise helps exploration)
- âœ… Online learning: Can update as new data arrives

**Cons:**
- âŒ Noisy convergence (loss bounces around)
- âŒ Harder to parallelize
- âŒ May not converge to exact minimum

**When to use:** Large datasets (N > 100,000), online learning

---

### **C. Mini-Batch Gradient Descent (MBGD)**

**Definition:** Use **small batch** of examples (e.g., 32, 64, 128)

```python
batch_size = 64
for epoch in range(num_epochs):
    shuffle(data)
    for batch in get_batches(data, batch_size):
        gradient = (1/batch_size) * Î£ compute_gradient(x, y, W)
        W = W - learning_rate * gradient
```

**Mathematical form:**
```
âˆ‡L(W) â‰ˆ (1/B) Î£â±¼â‚Œâ‚á´® âˆ‡Lâ±¼(W)  (average over B examples)
```

**Variance reduction:**
```
Var(mini-batch gradient) = Var(SGD) / âˆšB
```

**Pros:**
- âœ… **Best of both worlds**
- âœ… Less noisy than SGD, faster than BGD
- âœ… GPU-friendly (vectorized operations)
- âœ… Still explores (some noise remains)

**Cons:**
- âŒ Batch size is a hyperparameter to tune

**When to use:** **Always!** This is the industry standard.

**Typical batch sizes:**
- Small models: 32-64
- Standard: 128-256
- Large models/GPUs: 512-1024

---

### **Visual Comparison**

```
Loss Landscape:

BGD:   Smooth path
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ min

SGD:   Noisy zigzag
       â”€â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²â”€â†’ ~ min

MBGD:  Moderately smooth
       â”€â”€â•±â•²â”€â•±â•²â”€â”€â†’ min
```

---

## 4. Learning Rate Dynamics

### **The Most Critical Hyperparameter**

Learning rate `Î±` controls step size. **Getting this right is crucial.**

### **Problems with Fixed Learning Rate**

**Too large (`Î± = 1.0`):**
```
Step 0: W = [1.0, 2.0], Loss = 10.0
Step 1: W = [5.0, -3.0], Loss = 50.0  â† Overshot!
Step 2: W = [-10.0, 15.0], Loss = 200.0  â† Diverging!
Step 3: Loss = NaN  ğŸ’¥
```

**Too small (`Î± = 0.0001`):**
```
Step 0: Loss = 10.0
Step 100: Loss = 9.95  â† Barely moved
Step 1000: Loss = 9.5  â† Still far from minimum
```

**Just right (`Î± = 0.01`):**
```
Step 0: Loss = 10.0
Step 10: Loss = 5.0
Step 30: Loss = 1.0
Step 50: Loss = 0.1  âœ“
```

---

### **Learning Rate Schedules**

**Problem:** Ideal learning rate changes during training!

- **Early:** Large steps explore landscape
- **Late:** Small steps fine-tune around minimum

**Solution:** **Learning rate decay**

#### **1. Step Decay**

```
lr(t) = lrâ‚€ Ã— Î³^floor(t/drop_every)

Example: lrâ‚€=0.1, Î³=0.5, drop_every=10
  Epoch 0-9:   lr = 0.1
  Epoch 10-19: lr = 0.05
  Epoch 20-29: lr = 0.025
```

#### **2. Exponential Decay**

```
lr(t) = lrâ‚€ Ã— e^(-Î»t)

Smooth, continuous decay
```

#### **3. Polynomial Decay**

```
lr(t) = lrâ‚€ Ã— (1 - t/T)^p

Where T = total steps, p = power (usually 0.5 or 1)
```

#### **4. Cosine Annealing**

```
lr(t) = lr_min + 0.5(lrâ‚€ - lr_min)(1 + cos(Ï€t/T))

Smooth curve from lrâ‚€ to lr_min
Used in modern transformers
```

---

### **Warmup**

**Problem:** Large LR at start can destabilize training

**Solution:** Gradually increase LR

```
if epoch < warmup_epochs:
    lr = lr_max Ã— (epoch / warmup_epochs)
else:
    lr = lr_max Ã— decay_schedule(epoch - warmup_epochs)
```

**Used in:** BERT, GPT, Vision Transformers

---

## 5. Convergence Analysis

### **When Does GD Converge?**

**Theorem (Convex Case):**

If `L(W)` is convex and ` smooth, GD with `Î± < 2/L` converges to global minimum.

**Rate:** `O(1/T)` after T steps

**Reality:** Neural networks are **non-convex**, so this doesn't apply!

---

### **Non-Convex Case**

GD converges to a **critical point** where `âˆ‡L = 0`:
- Could be local minimum
- Could be saddle point
- Could be global minimum (lucky!)

**Key insight:** SGD noise helps escape saddle points!

---

### **Convergence Criteria**

**How to know when to stop?**

#### **1. Gradient Norm**
```
if ||âˆ‡L(W)|| < Îµ:
    converged = True
```

#### **2. Loss Change**
```
if |L(W_t) - L(W_{t-1})| < Îµ:
    converged = True
```

#### **3. Parameter Change**
```
if ||W_t - W_{t-1}|| < Îµ:
    converged = True
```

#### **4. Validation Loss (Practical)**
```
if val_loss not improved for k epochs:
    early_stop = True
```

---

## 6. Common Pitfalls & Solutions

### **Problem 1: Exploding Gradients**

**Symptoms:**
```
Epoch 1: Loss = 2.5
Epoch 2: Loss = 5.0
Epoch 3: Loss = 50.0
Epoch 4: Loss = NaN  ğŸ’¥
```

**Causes:**
- Learning rate too large
- Unstable initialization
- Deep networks (gradient multiplication)

**Solutions:**
```python
# 1. Gradient clipping
if grad_norm > threshold:
    gradient *= threshold / grad_norm

# 2. Reduce learning rate
learning_rate /= 10

# 3. Better initialization (Xavier, He)
W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)
```

---

### **Problem 2: Vanishing Gradients**

**Symptoms:**
```
Epoch 1: Loss = 2.5
Epoch 100: Loss = 2.49
Epoch 1000: Loss = 2.48  â† No progress!
```

**Causes:**
- Learning rate too small
- Poor activation functions (sigmoid squashes)
- Deep networks

**Solutions:**
- Increase learning rate
- Use ReLU instead of sigmoid
- Batch normalization
- Residual connections (ResNets)

---

### **Problem 3: Oscillation**

**Symptoms:**
```
Loss: 2.5 â†’ 1.5 â†’ 2.0 â†’ 1.3 â†’ 1.8 â†’ ...
      â†‘ Bouncing around minimum
```

**Cause:** Learning rate too large for fine-tuning

**Solution:** Learning rate decay

---

### **Problem 4: Slow Convergence**

**Symptoms:**
```
Epoch 1: Loss = 2.500
Epoch 100: Loss = 2.499
          â†‘ Tiny progress
```

**Causes:**
- Learning rate too small
- Poor conditioning (some directions have small gradients)

**Solutions:**
- Increase learning rate
- Use momentum
- Use adaptive optimizers (Adam)

---

## 7. Connection to Modern Optimizers

Vanilla GD is the foundation. Modern optimizers add:

### **Momentum**

**Problem:** GD oscillates in valleys

**Solution:** Add "velocity" term

```
v_{t+1} = Î²Â·v_t + âˆ‡L(W_t)
W_{t+1} = W_t - Î±Â·v_{t+1}
```

**Effect:** Smooths updates, accelerates convergence

---

### **RMSprop**

**Problem:** One learning rate for all parameters

**Solution:** Adapt learning rate per parameter

```
E[gÂ²]_t = Î²Â·E[gÂ²]_{t-1} + (1-Î²)Â·g_tÂ²
W_{t+1} = W_t - Î±/(âˆš(E[gÂ²]_t) + Îµ) Â· g_t
```

**Effect:** Large gradients â†’ small steps; small gradients â†’ large steps

---

### **Adam** (Most Popular)

**Combines momentum + RMSprop:**

```
m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t        (momentum)
v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²       (adaptive LR)

mÌ‚_t = m_t / (1 - Î²â‚áµ—)  (bias correction)
vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)

W_{t+1} = W_t - Î± Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

**Default hyperparameters:**
- `Î± = 0.001`
- `Î²â‚ = 0.9`
- `Î²â‚‚ = 0.999`

**Why Adam is popular:**
- âœ… Works well out-of-the-box
- âœ… Adaptive learning rates
- âœ… Momentum smoothing
- âœ… Used in 90% of papers

---

## ğŸ“Š Summary Table

| Method | Batch Size | Speed | Convergence | GPU-Friendly | Use Case |
|--------|-----------|-------|-------------|--------------|----------|
| BGD | N (all) | Slow | Smooth | No | Small data, convex |
| SGD | 1 | Fast | Noisy | No | Online learning |
| MBGD | 32-256 | Medium | Balanced | **Yes** | **Production** |
| +Momentum | 32-256 | Fast | Smooth | Yes | CNNs |
| +Adam | 32-256 | Fast | Smooth | Yes | **Default choice** |

---

## âœ… Key Takeaways

1. **Gradient descent is iterative hill descent**
   - Move in direction opposite to gradient
   - Guaranteed to decrease loss locally

2. **Three variants, one winner**
   - BGD: Exact but slow
   - SGD: Fast but noisy
   - **MBGD: Best balance** â† Industry standard

3. **Learning rate is critical**
   - Too large: diverges
   - Too small: slow
   - **Use decay schedules**

4. **Modern optimizers extend vanilla GD**
   - Momentum: Smooths updates
   - RMSprop: Adapts per parameter
   - **Adam: Combines both** â† Most popular

5. **Always monitor training**
   - Watch for NaN (exploding gradients)
   - Check convergence (early stopping)
   - Validate on held-out data

---

## ğŸ“š Further Reading

1. **Foundational Papers:**
   - Robbins & Monro (1951): *A Stochastic Approximation Method*
   - Polyak (1964): *Some methods of speeding up convergence of iterative methods*
   - Kingma & Ba (2014): *Adam: A Method for Stochastic Optimization*

2. **Modern Surveys:**
   - Ruder (2016): *An overview of gradient descent optimization algorithms*
   - Smith (2017): *Don't Decay the Learning Rate, Increase the Batch Size*

3. **CS231n Resources:**
   - [Optimization Notes](https://cs231n.github.io/optimization-1/)
   - [Neural Networks Part 3](https://cs231n.github.io/neural-networks-3/)

---

**End of Theory Notes** ğŸ“š