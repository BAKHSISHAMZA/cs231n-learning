# ðŸ“– Theory Notes: k-Nearest Neighbors Classifier

> **CS231n Lecture 2:** Image Classification  
> **Author:** Hamza  
> **Last Updated:** November 2024

---

## ðŸŽ¯ Table of Contents

1. [The Image Classification Problem](#the-image-classification-problem)
2. [k-Nearest Neighbors Algorithm](#k-nearest-neighbors-algorithm)
3. [Distance Metrics](#distance-metrics)
4. [Hyperparameter Selection](#hyperparameter-selection)
5. [Why kNN Fails for Images](#why-knn-fails-for-images)
6. [Mathematical Deep Dive](#mathematical-deep-dive)

---

## 1. The Image Classification Problem

### **Problem Formulation**

Given:
- **Input:** An image (represented as a grid of pixel values)
- **Output:** A label from a fixed set of categories

**Mathematically:**
```
f: â„^(HÃ—WÃ—C) â†’ {1, 2, ..., K}

Where:
  H = image height
  W = image width
  C = number of channels (3 for RGB)
  K = number of classes
```

### **Semantic Gap**

The challenge: Computers see images as arrays of numbers, but we want them to understand **semantic content**.

**Example:**
```
Human sees: "A cat sitting on a couch"
Computer sees: [[[123, 45, 67], [124, 46, 68], ...], ...]
```

### **Key Challenges**

| Challenge | Description | Example |
|-----------|-------------|---------|
| **Viewpoint variation** | Same object from different angles | Front-facing cat vs side-view cat |
| **Illumination** | Lighting affects pixel values drastically | Cat in sunlight vs shadow |
| **Deformation** | Objects can change shape | Cat sitting vs cat stretching |
| **Occlusion** | Parts may be hidden | Cat behind a plant |
| **Background clutter** | Objects blend with surroundings | Camouflaged cat |
| **Intra-class variation** | Same class, different appearances | Persian cat vs Siamese cat |

---

## 2. k-Nearest Neighbors Algorithm

### **Core Intuition**

> "You are the average of your k nearest neighbors."

If most of your neighbors belong to class A, you probably belong to class A too.

### **Algorithm Steps**

#### **Training Phase**
```
Input: Training set (X_train, y_train)
Output: Stored data

Simply memorize all training examples.
Time: O(1)
```

#### **Prediction Phase**
```
Input: Test example x_test
Output: Predicted label y_pred

1. Compute distance from x_test to all training examples
2. Find k nearest neighbors
3. Take majority vote among their labels
4. Return most common label

Time: O(N Ã— D)
  N = number of training examples
  D = dimensionality
```

### **Pseudocode**

```python
class kNN:
    def train(X_train, y_train):
        self.X = X_train
        self.y = y_train
    
    def predict(x_test, k):
        distances = compute_distance(x_test, self.X)
        k_indices = get_k_smallest(distances, k)
        k_labels = self.y[k_indices]
        return most_common(k_labels)
```

### **Time Complexity Analysis**

| Phase | Complexity | Why? |
|-------|-----------|------|
| **Training** | O(1) | Just store data in memory |
| **Prediction** | O(N Ã— D) | Compare test example to all N training examples |

**Problem for production:**
- We want: Fast at test time (real-time prediction)
- kNN gives: Fast training, **slow testing** âŒ

---

## 3. Distance Metrics

Distance metrics define "similarity" between data points.

### **L1 Distance (Manhattan Distance)**

**Formula:**
```
d_L1(Iâ‚, Iâ‚‚) = Î£_p |Iâ‚[p] - Iâ‚‚[p]|
```

**Intuition:** Sum of absolute differences across all pixels.

**Visual Analogy:**
```
Manhattan grid: You can only move along streets
Distance = total blocks traveled (horizontal + vertical)
```

**Example:**
```python
Iâ‚ = [10, 20, 30, 40]
Iâ‚‚ = [12, 18, 33, 38]

d_L1 = |10-12| + |20-18| + |30-33| + |40-38|
     = 2 + 2 + 3 + 2
     = 9
```

**Properties:**
- âœ“ Fast to compute
- âœ“ Coordinate-dependent (rotating image changes distance)
- âœ“ Robust to outliers (doesn't square differences)

---

### **L2 Distance (Euclidean Distance)**

**Formula:**
```
d_L2(Iâ‚, Iâ‚‚) = âˆš(Î£_p (Iâ‚[p] - Iâ‚‚[p])Â²)
```

**Intuition:** "As the crow flies" - straight-line distance.

**Example:**
```python
Iâ‚ = [10, 20, 30, 40]
Iâ‚‚ = [12, 18, 33, 38]

d_L2 = âˆš((10-12)Â² + (20-18)Â² + (30-33)Â² + (40-38)Â²)
     = âˆš(4 + 4 + 9 + 4)
     = âˆš21
     â‰ˆ 4.58
```

**Properties:**
- âœ“ Natural geometric interpretation
- âœ“ Rotation-invariant in the right coordinate system
- âœ— Sensitive to outliers (squares large differences)

---

### **Visual Comparison**

```
L1 distance:           L2 distance:
  â”Œâ”€â”€â”€â”€â”€â”               â•±â”€â”€â”€â”€â”€â•²
  â”‚     â”‚              â•±       â•²
  â”‚  â—â”€â”€â”¼â”€â”€â—         â—           â—
  â”‚     â”‚              â•²       â•±
  â””â”€â”€â”€â”€â”€â”˜               â•²â”€â”€â”€â”€â”€â•±
  
 (Diamond shape)      (Circle shape)
```

Points at distance d=1:
- **L1:** Form a diamond (Manhattan ball)
- **L2:** Form a circle (Euclidean ball)

---

### **RGB Images**

For color images with 3 channels (R, G, B):

```python
# Image shape: (H, W, 3)
# Flatten to 1D: (H Ã— W Ã— 3,)

# L1 distance
d_L1 = Î£_x Î£_y (|Râ‚[x,y] - Râ‚‚[x,y]| + 
                 |Gâ‚[x,y] - Gâ‚‚[x,y]| + 
                 |Bâ‚[x,y] - Bâ‚‚[x,y]|)

# L2 distance
d_L2 = âˆš(Î£_x Î£_y ((Râ‚[x,y] - Râ‚‚[x,y])Â² + 
                   (Gâ‚[x,y] - Gâ‚‚[x,y])Â² + 
                   (Bâ‚[x,y] - Bâ‚‚[x,y])Â²))
```

---

## 4. Hyperparameter Selection

### **What are Hyperparameters?**

**Hyperparameters:** Choices we make **before** training (not learned from data).

For kNN:
- `k`: Number of neighbors to consider
- Distance metric: L1, L2, cosine, etc.

### **The Golden Rule**

> **NEVER tune hyperparameters on test data!**

**Why?**
- Test set must simulate "never-before-seen" data
- If you optimize for test performance, you're **cheating**
- Real-world performance will be worse

---

### **Train/Validation/Test Split**

**Correct procedure:**

```
Full Dataset
    â”‚
    â”œâ”€â”€ 80% Training Set
    â”‚   â””â”€â”€ Train model parameters
    â”‚
    â”œâ”€â”€ 10% Validation Set
    â”‚   â””â”€â”€ Choose hyperparameters (k, distance metric)
    â”‚
    â””â”€â”€ 10% Test Set
        â””â”€â”€ Final evaluation (run ONCE)
```

**Workflow:**

```python
# 1. Split data
X_train, X_val, X_test = split_data(X)

# 2. Try different hyperparameters on VALIDATION set
best_k = None
best_accuracy = 0

for k in [1, 3, 5, 10, 20]:
    model = kNN(k=k)
    model.train(X_train, y_train)
    accuracy = model.score(X_val, y_val)
    
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_k = k

# 3. Train final model with best_k
final_model = kNN(k=best_k)
final_model.train(X_train, y_train)

# 4. Evaluate on TEST set (ONCE!)
test_accuracy = final_model.score(X_test, y_test)
print(f"Final test accuracy: {test_accuracy}")
```

---

### **Cross-Validation**

For small datasets, use **k-fold cross-validation**:

```
Fold 1: [Val | Train | Train | Train | Train]
Fold 2: [Train | Val | Train | Train | Train]
Fold 3: [Train | Train | Val | Train | Train]
Fold 4: [Train | Train | Train | Val | Train]
Fold 5: [Train | Train | Train | Train | Val]

Average validation performance across all folds
```

**Benefit:** Use all data for both training and validation.

---

## 5. Why kNN Fails for Images

### **Problem 1: Computational Cost**

- **Test time:** O(N Ã— D) per image
  - CIFAR-10: N = 50,000, D = 32Ã—32Ã—3 = 3,072
  - **1 image = 150M operations** ðŸ˜±
  
- **Unacceptable for production:** Real-time systems need <10ms per image

---

### **Problem 2: Perceptual Distance â‰  Pixel Distance**

**Example: Image shifts**

```
Original:        Shifted 1 pixel:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 0 â”‚100â”‚200â”‚    â”‚100â”‚200â”‚ 0 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤ vs â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 0 â”‚100â”‚200â”‚    â”‚100â”‚200â”‚ 0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

L2 distance: LARGE (almost all pixels changed!)
Perceptual similarity: HIGH (visually identical)
```

**Conclusion:** Raw pixel distance doesn't capture semantic similarity.

---

### **Problem 3: Curse of Dimensionality**

In high dimensions, all points become **equally distant**.

**Example:** Random points in d-dimensional hypercube

| Dimensions | Avg distance | Insight |
|------------|--------------|---------|
| 2D | Varies | Neighbors are meaningful |
| 10D | More uniform | Neighbors start losing meaning |
| 100D | Nearly identical | All points are "far" |
| 3072D (CIFAR) | **All points equidistant** | kNN breaks down |

**Mathematical intuition:**
```
In d dimensions, volume of hypersphere ~ r^d

To cover constant fraction of space, need:
N_samples ~ exp(d)  (exponential growth!)
```

---

## 6. Mathematical Deep Dive

### **Why Does kNN Work? (Statistical View)**

**Theorem (Cover & Hart, 1967):**

As N â†’ âˆž, 1-NN error rate â‰¤ 2 Ã— Bayes error rate

**Intuition:**
- With infinite data, nearest neighbor captures local density
- Optimal decision boundary is recovered in the limit

**Reality:**
- We don't have infinite data
- Curse of dimensionality makes "neighbors" meaningless in high dims

---

### **Distance Metrics as Inner Products**

**L2 distance can be rewritten:**
```
â€–x - yâ€–â‚‚Â² = (x - y)áµ€(x - y)
          = xáµ€x - 2xáµ€y + yáµ€y
          = â€–xâ€–Â² - 2âŸ¨x, yâŸ© + â€–yâ€–Â²
```

**Key insight:** Distance depends on **inner product** âŸ¨x, yâŸ©

This connects to:
- Kernel methods (SVMs)
- Neural network similarity learning
- Modern metric learning

---

### **When kNN Actually Works**

**Good use cases:**
1. **Low-dimensional data** (d < 20)
2. **Large datasets** relative to dimensionality
3. **Meaningful distance metrics** (not raw pixels)

**Modern usage:**
```
Deep Learning Features + kNN
           â†“
1. Use CNN to extract features (e.g., ResNet)
   Image (224Ã—224Ã—3) â†’ Embedding (512,)
2. Apply kNN on embeddings
3. Embeddings capture semantic similarity!
```

This is used in:
- Image retrieval systems
- Few-shot learning
- Anomaly detection

---

## ðŸ“š Summary

### **Key Takeaways**

1. âœ… **kNN is simple:** Train = memorize, Predict = find neighbors
2. âœ… **Distance metric matters:** L1 vs L2 is a hyperparameter choice
3. âœ… **Validation set is essential:** Never tune on test data
4. âŒ **kNN fails for raw images:** Pixel distance â‰  perceptual distance
5. âŒ **Curse of dimensionality:** Need exponentially more data in high dims
6. âœ… **Modern use:** kNN on learned features (CNN embeddings) works great

### **What's Next?**

kNN taught us:
- Classification pipeline (train/val/test)
- Importance of distance metrics
- Need for better representations

**Next:** Linear classifiers - our first **parametric** model!
- Instead of memorizing data, **learn parameters**
- Fast at test time (O(D) instead of O(NÃ—D))
- Foundation for neural networks

---

## ðŸ“– References

1. Cover, T. M., & Hart, P. E. (1967). *Nearest neighbor pattern classification.* IEEE Transactions on Information Theory.
2. Weinberger, K. Q., & Saul, L. K. (2009). *Distance metric learning for large margin nearest neighbor classification.* JMLR.
3. CS231n Course Notes: [Image Classification](https://cs231n.github.io/classification/)

---

**End of Theory Notes** ðŸ“š